# ============================================================================
# PARAMETROS DEL PIPELINE REGRESION_DATA
# ============================================================================

regresion_data:
  # Rango temporal (era moderna)
  modern_era_start: 2010
  modern_era_end: 2024

  # Variables prohibidas (data leakage)
  leakage_features:
    - positionText
    - positionOrder
    - points
    - time
    - milliseconds
    - rank
    - fastestLap
    - fastestLapTime
    - fastestLapSpeed
    - statusId
    - laps
    - resultId
    - number
    - q1
    - q2
    - q3

  # Split train/test
  test_size: 0.20
  random_state: 42

# ============================================================================
# PARAMETROS DEL PIPELINE REGRESION_MODELS
# ============================================================================

regresion_models:
  # Random state para reproducibilidad
  random_state: 42

  # Grids de GridSearch para cada modelo
  gridsearch_grids:
    GradientBoosting:
      n_estimators: [100, 150, 200]
      learning_rate: [0.05, 0.1, 0.15]
      max_depth: [3, 5, 7]
      min_samples_split: [2, 5]
      subsample: [0.8, 1.0]

    Ridge:
      alpha: [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]

    LightGBM:
      n_estimators: [100, 150, 200]
      learning_rate: [0.05, 0.1, 0.15]
      num_leaves: [31, 50, 70]
      max_depth: [5, 7, 10]
      min_child_samples: [20, 30]

    CatBoost:
      iterations: [100, 150, 200]
      learning_rate: [0.05, 0.1, 0.15]
      depth: [4, 6, 8]
      l2_leaf_reg: [1, 3, 5]

    RandomForest:
      n_estimators: [100, 150, 200]
      max_depth: [10, 15, 20, null]
      min_samples_split: [2, 5, 10]
      min_samples_leaf: [1, 2, 4]
      max_features: [sqrt, log2]

# ============================================================================
# PARAMETROS DEL PIPELINE CLASSIFICATION_DATA
# ============================================================================

classification_data:
  # Fecha de corte train/test (split temporal)
  temporal_split_date: "2023-01-01"

  # Variables prohibidas (data leakage)
  leakage_features:
    - position
    - positionOrder
    - positionText
    - points
    - time
    - milliseconds
    - statusId
    - laps
    - rank
    - fastestLap
    - fastestLapTime
    - fastestLapSpeed

  # Random state
  random_state: 42

  # Features para MinMaxScaler
  minmax_features:
    - grid
    - round
    - season_progress
    - driver_podiums_last_5
    - constructor_podiums_last_5
    - driver_podium_rate_season
    - constructor_podium_rate_season
    - q1_ms
    - q2_ms
    - q3_ms

  # Features para StandardScaler
  standard_features:
    - lat
    - lng
    - alt
    - year
    - driver_age

# ============================================================================
# PARAMETROS DEL PIPELINE CLASSIFICATION_MODELS
# ============================================================================

classification_models:
  # Random state
  random_state: 42

  # Configuraci√≥n SMOTE
  smote_sampling_strategy: 0.5

  # Scoring principal y CV
  scoring: 'f1'
  cv_folds: 5

  # Grids de GridSearch para cada modelo
  gridsearch_grids:
    LogisticRegression:
      C: [0.001, 0.01, 0.1, 1, 10]
      penalty: ['l2']
      solver: ['lbfgs', 'liblinear']
      max_iter: [1000]

    RandomForest:
      n_estimators: [100, 200, 300]
      max_depth: [10, 20, 30, null]
      min_samples_split: [2, 5, 10]
      min_samples_leaf: [1, 2, 4]
      class_weight: ['balanced', null]

    XGBoost:
      n_estimators: [100, 200, 300]
      max_depth: [3, 5, 7, 10]
      learning_rate: [0.01, 0.05, 0.1, 0.2]
      subsample: [0.8, 1.0]
      colsample_bytree: [0.8, 1.0]

    LightGBM:
      n_estimators: [100, 200, 300]
      num_leaves: [31, 50, 100]
      learning_rate: [0.01, 0.05, 0.1]
      min_child_samples: [20, 30, 50]
      is_unbalance: [true, false]

    CatBoost:
      iterations: [100, 200, 300]
      depth: [4, 6, 8]
      learning_rate: [0.01, 0.05, 0.1]
      l2_leaf_reg: [1, 3, 5]
      auto_class_weights: ['Balanced', null]

    GradientBoosting:
      n_estimators: [100, 200]
      max_depth: [3, 5, 7]
      learning_rate: [0.01, 0.1]
      subsample: [0.8, 1.0]
      min_samples_split: [2, 5]
