# ============================================================================
# PARAMETROS DEL PIPELINE REGRESION_DATA
# ============================================================================

regresion_data:
  # Rango temporal (era moderna)
  modern_era_start: 2010
  modern_era_end: 2024

  # Variables prohibidas (data leakage)
  leakage_features:
    - positionText
    - positionOrder
    - points
    - time
    - milliseconds
    - rank
    - fastestLap
    - fastestLapTime
    - fastestLapSpeed
    - statusId
    - laps
    - resultId
    - number
    - q1
    - q2
    - q3

  # Split train/test
  test_size: 0.20
  random_state: 42

# ============================================================================
# PARAMETROS DEL PIPELINE REGRESION_MODELS
# ============================================================================

regresion_models:
  # Random state para reproducibilidad
  random_state: 42

  # Grids de GridSearch para cada modelo (reducidos para evitar OOM)
  gridsearch_grids:
    GradientBoosting:
      n_estimators: [100, 150]
      learning_rate: [0.05, 0.1]
      max_depth: [3, 5]
      min_samples_split: [2, 5]
      subsample: [0.8, 1.0]
      # Combinaciones: 2×2×2×2×2 = 32

    Ridge:
      alpha: [0.01, 0.1, 1.0, 10.0]
      # Combinaciones: 4

    LightGBM:
      n_estimators: [100, 150]
      learning_rate: [0.05, 0.1]
      num_leaves: [31, 50]
      max_depth: [5, 7]
      min_child_samples: [20]
      # Combinaciones: 2×2×2×2×1 = 16

    CatBoost:
      iterations: [100, 150]
      learning_rate: [0.05, 0.1]
      depth: [4, 6]
      l2_leaf_reg: [1, 3]
      # Combinaciones: 2×2×2×2 = 16

    RandomForest:
      n_estimators: [100, 150]
      max_depth: [10, 20, null]
      min_samples_split: [2, 5]
      min_samples_leaf: [1, 2]
      max_features: [sqrt]
      # Combinaciones: 2×3×2×2×1 = 24

# ============================================================================
# PARAMETROS DEL PIPELINE CLASSIFICATION_DATA
# ============================================================================

classification_data:
  # Fecha de corte train/test (split temporal)
  temporal_split_date: "2023-01-01"

  # Variables prohibidas (data leakage)
  leakage_features:
    - position
    - positionOrder
    - positionText
    - points
    - time
    - milliseconds
    - statusId
    - laps
    - rank
    - fastestLap
    - fastestLapTime
    - fastestLapSpeed

  # Random state
  random_state: 42

  # Features para MinMaxScaler
  minmax_features:
    - grid
    - round
    - season_progress
    - driver_podiums_last_5
    - constructor_podiums_last_5
    - driver_podium_rate_season
    - constructor_podium_rate_season
    - q1_ms
    - q2_ms
    - q3_ms

  # Features para StandardScaler
  standard_features:
    - lat
    - lng
    - alt
    - year
    - driver_age

# ============================================================================
# PARAMETROS DEL PIPELINE CLASSIFICATION_MODELS
# ============================================================================

classification_models:
  # Random state
  random_state: 42

  # Configuración SMOTE
  smote_sampling_strategy: 0.5

  # Scoring principal y CV (reducido a 3 para evitar OOM)
  scoring: 'f1'
  cv_folds: 3

  # Grids de GridSearch para cada modelo (reducidos para evitar OOM)
  gridsearch_grids:
    LogisticRegression:
      C: [0.01, 0.1, 1, 10]
      penalty: ['l2']
      solver: ['lbfgs']
      max_iter: [1000]
      # Combinaciones: 4×1×1×1 = 4

    RandomForest:
      n_estimators: [100, 200]
      max_depth: [10, 20, null]
      min_samples_split: [2, 5]
      min_samples_leaf: [1, 2]
      class_weight: ['balanced', null]
      # Combinaciones: 2×3×2×2×2 = 48

    XGBoost:
      n_estimators: [100, 200]
      max_depth: [3, 5, 7]
      learning_rate: [0.05, 0.1]
      subsample: [0.8, 1.0]
      colsample_bytree: [0.8]
      # Combinaciones: 2×3×2×2×1 = 24

    LightGBM:
      n_estimators: [100, 200]
      num_leaves: [31, 50]
      learning_rate: [0.05, 0.1]
      min_child_samples: [20, 30]
      is_unbalance: [true]
      # Combinaciones: 2×2×2×2×1 = 16

    CatBoost:
      iterations: [100, 200]
      depth: [4, 6]
      learning_rate: [0.05, 0.1]
      l2_leaf_reg: [1, 3]
      auto_class_weights: ['Balanced']
      # Combinaciones: 2×2×2×2×1 = 16

    GradientBoosting:
      n_estimators: [100, 200]
      max_depth: [3, 5]
      learning_rate: [0.05, 0.1]
      subsample: [0.8, 1.0]
      min_samples_split: [2]
      # Combinaciones: 2×2×2×2×1 = 16
